{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面这戏代码实现了划分数据，计算最小值，最大值，数据缩放和训练SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#加载并划分数据\n",
    "cancer = load_breast_cancer()\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "#计算训练数据的最大最小值\n",
    "scaler = MinMaxScaler().fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对训练数据进行缩放\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "svm = SVC(gamma='auto')\n",
    "#SVM在缩放后的数据上学习\n",
    "svm.fit(x_train_scaled, y_train)\n",
    "#对测试集进行缩放，并进行打分\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "svm.score(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用pipeline类来简化构建变换和模型链的过程。如何将pipeline和gridsearchcv结合起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1用预处理进行参数选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9812206572769953\n",
      "0.972027972027972\n",
      "{'C': 1, 'gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "#这个网格搜索有个不易察觉的陷阱，我们使用缩放后的所有训练数据来运行带交叉验证的网格搜索，交叉验证中的划分\n",
    "#无法正确的反应新数据的特征，我们已经将这部分数据的信息泄露给建模过程，交叉验证结果将过于乐观\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'C':[0.001,0.01,0.1,1,10,100],'gamma':[0.001,0.01,0.1,1,10,100]}\n",
    "grid=GridSearchCV(SVC(),param_grid=param_grid,cv=5)\n",
    "grid.fit(x_train_scaled,y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.score(x_test_scaled,y_test))\n",
    "print(grid.best_params_)\n",
    "#不要在实践中使用这个方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在交叉验证的过程中，应该在进行任何预处理之前完成数据集的划分。任何从数据集中提取信息的处理过程都应该仅应用于数据集的训练部分。任何交叉验证都应该位于处理过程的“最外层循环”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline类最常见的用例是将预处理步骤（比如数据缩放）与一个监督模型（比如分类器）连接在一起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2构建管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们先使用MinMaxScaler缩放数据之后再训练一个SVM的工作流程（暂不使用网格搜索）\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([(\"scaler\",MinMaxScaler()),('svm',SVC(gamma='auto'))])\n",
    "pipe.fit(x_train,y_train)\n",
    "pipe.score(x_test,y_test)\n",
    "#利用管道，我们减少了‘预处理+分类’过程所需要的代码量\n",
    "#使用管道的主要优点在于，我们可以在cross_val_score或GridSearchCV中使用这个估计器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3在网格搜索中使用管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9812206572769953\n",
      "0.972027972027972\n",
      "{'svm__C': 1, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(x_train, y_train)\n",
    "print((grid.best_score_))\n",
    "print((grid.score(x_test, y_test)))\n",
    "print((grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这次交叉验证的每次划分，仅使用训练部分对MinMaxScaler进行拟合，测试部分信息没有泄露到建模过程中（参数搜索中）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息泄露举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rnd=np.random.RandomState(seed=0)\n",
    "x=rnd.normal(size=(100,10000))\n",
    "y=rnd.normal(size=(100,))\n",
    "from sklearn.feature_selection import SelectPercentile,f_regression\n",
    "select=SelectPercentile(score_func=f_regression,percentile=5).fit(x,y)\n",
    "x_selected=select.transform(x)\n",
    "x_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84834054, 0.94084243, 0.88541709, 0.94012139, 0.91425508])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "cross_val_score(Ridge(),x_selected,y,cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x和y是随机数本质上没任何关系，由于我们在交叉验证之外特征选择进行拟合，测试部分泄露出去的信息量非常大，导致结果不切实际"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.97502994, -0.03166358, -0.03989415,  0.03018385, -0.2163673 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe=Pipeline([(\"selectpercentile\",SelectPercentile(score_func=f_regression,percentile=5)),('ridge',Ridge())])\n",
    "cross_val_score(pipe,x,y,cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r-squared 为负表明模型很差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4通用的管道接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline类不但可以用于预处理和分类，实际上还可以将任意数量的估计器连接在一起，对于估计器的唯一要求是，除了最后一步外的所有步骤都需要具有transform方法，可以生成新的数据，以供下一个步骤使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.1用make_pipeline方便地创造管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#有一个很方便的函数make_pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#标准语法\n",
    "pipe_long=Pipeline([('scaler',MinMaxScaler()),('svm',SVC(C=100))])\n",
    "#缩写语法\n",
    "pipe_short=make_pipeline(MinMaxScaler(),SVC(C=100))\n",
    "#管道对象pipe_long,pipe_short作用完全相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('svc', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "      kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "      shrinking=True, tol=0.001, verbose=False))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_short.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.2访问步骤属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standardscaler-1',\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('pca',\n",
       "  PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "      svd_solver='auto', tol=0.0, whiten=False)),\n",
       " ('standardscaler-2',\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#通常检查管道中某一个步骤的属性，可以通过named_steps属性\n",
    "pipe.fit(cancer.data)\n",
    "components=pipe.named_steps['pca'].components_\n",
    "components.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.3访问网格搜索管道中的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('logisticregression',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                     warm_start=False))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe=make_pipeline(StandardScaler(),LogisticRegression())\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=0.1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings;warnings.simplefilter('ignore')\n",
    "param_grid={'logisticregression__C':[0.01,0.1,1,10,100]}\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=4)\n",
    "grid=GridSearchCV(pipe,param_grid,cv=5)\n",
    "grid.fit(x_train,y_train)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gridsearchcv找到的最佳模型（在所有训练数据上训练得到的模型）保存在grid.best_estimator_中\n",
    "#best_estimator_是一个管道，包含standardscaler和logisticregression两个步骤\n",
    "#我们可以使用管道的named_steps属性来访问logisticregression\n",
    "grid.best_estimator_.named_steps['logisticregression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.38856355, -0.37529972, -0.37624793, -0.39649439, -0.11519359,\n",
       "         0.01709608, -0.3550729 , -0.38995414, -0.05780518,  0.20879795,\n",
       "        -0.49487753, -0.0036321 , -0.37122718, -0.38337777, -0.04488715,\n",
       "         0.19752816,  0.00424822, -0.04857196,  0.21023226,  0.22444999,\n",
       "        -0.54669761, -0.52542026, -0.49881157, -0.51451071, -0.39256847,\n",
       "        -0.12293451, -0.38827425, -0.4169485 , -0.32533663, -0.13926972]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.named_steps['logisticregression'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5网格搜索预处理步骤与模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polynomialfeatures__degree': 2, 'ridge__alpha': 10}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target,random_state=0)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = make_pipeline(StandardScaler(),PolynomialFeatures(),Ridge())\n",
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid=GridSearchCV(pipe,param_grid=param_grid,cv=5,n_jobs=-1)\n",
    "grid.fit(x_train,y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7683045464100141"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时搜索预处理参数与模型参数是一个非常强大的策略，但是向网格中添加更多参数，需要构建的模型数量将呈指数增长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6网格搜索选择使用哪个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.01, 'preprocessing': StandardScaler(copy=True, with_mean=True, with_std=True)}\n",
      "0.9859154929577465\n",
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "#我们先定义管道。这里我们显示地对步骤命名，我们需要两个步骤，一个用于预处理，一个是分类器\n",
    "pipe=Pipeline([('preprocessing',StandardScaler()),('classifier',SVC())])\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = [\n",
    "    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\n",
    "     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    {'classifier': [RandomForestClassifier(n_estimators=100)],\n",
    "     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "grid=GridSearchCV(pipe,param_grid,cv=5)\n",
    "grid.fit(x_train,y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.score(x_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
